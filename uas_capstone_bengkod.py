# -*- coding: utf-8 -*-
"""UAS_Capstone_Bengkod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rypk3ul47aPOK2IFBDKiuvo69FV435Ka

Nama : Jonathan Karel Setiawan

NIM : A11.2020.13053

Link YT :

# 1. Pengumpulan Data
"""

# Import lib yang perlu

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Baca dataset

water_potability_df = pd.read_csv('https://drive.google.com/uc?id=1cWZzfVkEw3xMcIanmtb-xi-7nVBKjMRl')

water_potability_df.head()

"""# 2. Menelaah Data"""

# Tampilkan informasi data

water_potability_df.info()

# Tampilkan deskripsi data

water_potability_df.describe()

# Menampilkan jumlah missing values di setiap kolom

water_potability_df.isnull().sum()

# Menampilkan jumlah nilai unik di setiap kolom

water_potability_df.nunique()

# Menampilkan type data tiap kolom

water_potability_df.dtypes

"""# 3. Validasi dan Visualisasi Data

## Missing Value
"""

# Cek missing values

missing_values = water_potability_df.isnull().sum()
print("Missing Values:\n", missing_values)

# Mengisi kolom numerik dengan mean

water_potability_df.fillna(water_potability_df.mean(), inplace=True)

water_potability_df.isnull().sum()

"""## Outlier"""

def grab_col_names(dataframe, cat_th = 10, car_th = 20): #  memisahkan kolom dalam dataset berdasarkan tipe dan karakteristiknya.

    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object", "bool"]] # kolom kategorikal (tipe "category", "object", "bool", atau numerik dengan unique values < cat_th).
    num_but_cat = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"] and dataframe[col].nunique() < cat_th] # kolom numerik yang bertindak seperti kategorikal (berdasarkan jumlah unique values).
    cat_but_car = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object"] and dataframe[col].nunique() > car_th] # kolom kategorikal dengan unique values > car_th (high cardinality).

    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # kolom numerik murni (int64, float64) yang bukan kategorikal.
    num_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"]]
    num_cols = [col for col in num_cols if col not in cat_cols]

    print(f"Jumlah observasi: {dataframe.shape[0]}")
    print(f"Jumlah variabel: {dataframe.shape[1]}")
    print(f"Kolom kategorikal: {len(cat_cols)}")
    print(f"Kolom Numerik: {len(num_cols)}")
    print(f"Kategori tapi kardinal: {len(cat_but_car)}")
    print(f"Numerik tapi kategorikal: {len(num_but_cat)}")

    # mengembalikan daftar kolom yang dikelompokkan berdasarkan tipe (cat_cols, num_cols, cat_but_car).
    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = grab_col_names(water_potability_df)

# CEK OUTLIER
# Membuat visualisasi boxplot (diagram kotak) untuk semua kolom numerik yang diberikan
def boxplot(dataframe, numeric_columns):

    num_plots = len(numeric_columns)
    num_rows = (num_plots + 3) // 4 #Menghitung Jumlah Baris Grafik
    fig, axes = plt.subplots(num_rows, 4, figsize=(18, 4*num_rows)) # Membuat grid subplots dengan ukuran yang sesuai

    for i, column in enumerate(numeric_columns):
        row = i // 4
        col = i % 4
        sns.boxplot(y=dataframe[column], ax=axes[row, col]) # Iterasi Kolom Numerik untuk Membuat Boxplot
        axes[row, col].set_title(f'Boxplot of {column}')
        axes[row, col].set_ylabel(column)

    # Penentuan posisi subplot dalam grid
    for j in range(num_plots, num_rows*4):
        row = j // 4
        col = j % 4
        fig.delaxes(axes[row, col]) # Menghapus Grafik Kosong

    # Penyesuaian Layout
    plt.tight_layout()
    plt.show()

boxplot(water_potability_df, num_cols)

def detect_outlier_iqr_all_columns(df):

    outliers_dict = {}

    # Looping untuk setiap kolom numerik dalam DataFrame
    for col in df.select_dtypes(include=[np.number]).columns:
        data = df[col].dropna()  # Mengambil data dari kolom dan menghapus nilai yang hilang
        data_sorted = sorted(data)  # Mengurutkan data untuk menghitung kuantil

        # Menghitung Q1, Q3, dan IQR
        q1 = np.percentile(data_sorted, 25)
        q3 = np.percentile(data_sorted, 75)
        IQR = q3 - q1

        # Menentukan batas bawah dan atas untuk outlier
        lwr_bound = q1 - (1.5 * IQR)
        upr_bound = q3 + (1.5 * IQR)

        # Mencari outlier dan menyimpannya ke dalam list
        outliers = [i for i in data_sorted if i < lwr_bound or i > upr_bound]

        # Jika ada outlier, tambahkan ke kamus outliers_dict
        if outliers:
            outliers_dict[col] = outliers

    return outliers_dict

outliers_found = detect_outlier_iqr_all_columns(water_potability_df)

for col, outliers in outliers_found.items():
    print(f"Outliers pada kolom {col}  (Jumlah: {len(outliers)})")

"""## Resampling

Menggunakan metode Oversampling untuk menghindari model yang cenderung memprediksi kelas mayoritas dan mengabaikan kelas minoritas, yang kurang terwakili
"""

from imblearn.over_sampling import SMOTE

X = water_potability_df.drop('Potability', axis=1)  # Fitur
y = water_potability_df['Potability']  # Target

smote = SMOTE(random_state=42)  # Inisialisasi SMOTE dengan random state
X_resampled, y_resampled = smote.fit_resample(X, y)  # Resampling data

resampled_data = pd.DataFrame(X_resampled, columns=X.columns)
resampled_data['Potability'] = y_resampled

# Sebelum resampling
before_resampling = water_potability_df['Potability'].value_counts()

# Setelah resampling
after_resampling = resampled_data['Potability'].value_counts()

comparison_df = pd.DataFrame({
    'Sebelum Resampling': before_resampling,
    'Setelah Resampling': after_resampling
})
comparison_df.index = ['Tidak Layak Minum (0)', 'Layak Minum (1)']

comparison_df.plot(kind='bar', figsize=(10, 6))
plt.title('Perbandingan Distribusi Data Sebelum dan Sesudah Resampling')
plt.xlabel('Kualitas Air')
plt.ylabel('Jumlah Sampel')
plt.xticks(rotation=0)
plt.legend(loc='upper right')
plt.show()

"""# Menentukan Objek Data"""

from sklearn.model_selection import train_test_split

# Fitur (X)
X = water_potability_df[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']]

# Target (y)
y = water_potability_df['Potability']

# Membagi data menjadi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Membersihkan Data"""

# Menghitung matriks korelasi
correlation = water_potability_df.corr()

# Membuat heatmap
plt.figure(figsize=(15, 12))  # Atur ukuran gambar jika diperlukan
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap Korelasi Antar Kolom')
plt.show()

# Fitur yang ingin divisualisasikan
features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']

fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # 3 baris, 3 kolom

# Flatten axes untuk iterasi yang lebih mudah
axes = axes.flatten()

# Loop melalui setiap fitur dan buat histogram di subplot
for i, feature in enumerate(features):
    sns.histplot(data=water_potability_df, x=feature, kde=True, ax=axes[i])  # Buat histogram plot dengan seaborn di subplot
    axes[i].set_title(f'Distribusi {feature}')  # Atur judul plot
    axes[i].set_xlabel(feature)  # Atur label sumbu x
    axes[i].set_ylabel('Frekuensi')  # Atur label sumbu y

# Menyesuaikan layout dan menampilkan plot
plt.tight_layout()  # Menyesuaikan jarak antar subplot
plt.show()

"""# 6. Kontruksi Data

Salah satu tujuan dari tahapan konstruksi data yaitu untuk menyesuaikan semua tipe data yang
terdapat dalam dataset. Namun, pada tahap ini dataset sudah memiliki tipe data yang sesuai
sehingga tidak perlu dilakukan penyesuaian kembali

# 7. Pemodelan
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

clean_classifier_nb = GaussianNB()
clean_classifier_nb .fit(X_train, y_train)

clean_classifier_dt = DecisionTreeClassifier(random_state=42)
clean_classifier_dt .fit(X_train, y_train)

clean_classifier_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clean_classifier_rf .fit(X_train, y_train)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix,precision_score

def evaluation(Y_test,Y_pred):
    acc = accuracy_score(Y_test,Y_pred)
    rcl = recall_score(Y_test,Y_pred,average = 'weighted')
    f1 = f1_score(Y_test,Y_pred,average = 'weighted')
    ps = precision_score(Y_test,Y_pred,average = 'weighted')

    metric_dict={'accuracy': round(acc,3),
               'recall': round(rcl,3),
               'F1 score': round(f1,3),
               'Precision score': round(ps,3)
              }

    return print(metric_dict)

y_pred_nb = clean_classifier_nb.predict(X_test)

# Evaluate the Naive Bayes model
print("\nNaive Bayes Model:")
accuracy_nb = round(accuracy_score(y_test, y_pred_nb),3)
print("Accuracy:",accuracy_nb)
print("Classification Report:")
print(classification_report(y_test, y_pred_nb))

evaluation(y_test,y_pred_nb)

cm_nb = confusion_matrix(y_test, y_pred_nb)

# Membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

y_pred_dt = clean_classifier_dt.predict(X_test)

# Evaluate the Random Forest model
print("\nRandom Forest Model:")
accuracy_dt = round(accuracy_score(y_test, y_pred_dt),3)
print("Accuracy:",accuracy_dt)
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))

evaluation(y_test,y_pred_dt)

cm_dt = confusion_matrix(y_test, y_pred_dt)

# Membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

y_pred_rf = clean_classifier_rf.predict(X_test)

# Evaluate the Random Forest model
print("\nRandom Forest Model:")
accuracy_rf = round(accuracy_score(y_test, y_pred_rf),3)
print("Accuracy:",accuracy_rf)
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))

evaluation(y_test,y_pred_rf)

cm_rf = confusion_matrix(y_test, y_pred_rf)

# Membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

"""# 8. Evaluasi"""

model_comp = pd.DataFrame({'Model': ['Naive Bayes','Decision Tree','Random Forest'], 'Accuracy': [accuracy_nb*100,
                    accuracy_dt*100,accuracy_rf*100]})

# Membuat bar plot dengan keterangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_comp['Model'], model_comp['Accuracy'], color=['red', 'green', 'blue'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Sebelum di Normalisasi')
plt.xticks(rotation=45, ha='right')  # Untuk memutar label sumbu x agar lebih mudah dibaca

# Menambahkan keterangan jumlah di atas setiap bar
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

"""## Normalisasi Data"""

from sklearn.preprocessing import StandardScaler

# Inisialisasi StandardScaler
scaler = StandardScaler()

# Melakukan fit dan transform pada data training
X_train_scaled = scaler.fit_transform(X_train)

# Melakukan transform pada data testing (menggunakan parameter yang dipelajari dari data training)
X_test_scaled = scaler.transform(X_test)

# --- Sebelum Normalisasi ---
# Akurasi model sebelum normalisasi (sudah dihitung sebelumnya)
accuracy_nb = round(accuracy_score(y_test, y_pred_nb), 3)
accuracy_dt = round(accuracy_score(y_test, y_pred_dt), 3)
accuracy_rf = round(accuracy_score(y_test, y_pred_rf), 3)

# --- Setelah Normalisasi ---
# Prediksi model setelah normalisasi (sudah dihitung sebelumnya)
y_pred_nbN = clean_classifier_nb.predict(X_test_scaled)
y_pred_dtN = clean_classifier_dt.predict(X_test_scaled)
y_pred_rfN = clean_classifier_rf.predict(X_test_scaled)

# Akurasi model setelah normalisasi
accuracy_nbN = round(accuracy_score(y_test, y_pred_nbN), 3)
accuracy_dtN = round(accuracy_score(y_test, y_pred_dtN), 3)
accuracy_rfN = round(accuracy_score(y_test, y_pred_rfN), 3)

# --- Perbandingan Akurasi ---
# Membuat DataFrame untuk perbandingan
data = {
    'Model': ['Naive Bayes', 'Decision Tree', 'Random Forest'],
    'Akurasi Sebelum Normalisasi': [accuracy_nb, accuracy_dt, accuracy_rf],
    'Akurasi Setelah Normalisasi': [accuracy_nbN, accuracy_dtN, accuracy_rfN]
}

comparison_df = pd.DataFrame(data)

# Menampilkan DataFrame
print(comparison_df)

models = ['Naive Bayes', 'Decision Tree', 'Random Forest']
before_normalization = [accuracy_nb, accuracy_dt, accuracy_rf]
after_normalization = [accuracy_nbN, accuracy_dtN, accuracy_rfN]

# Menentukan posisi bar
x = np.arange(len(models))
width = 0.35

# Membuat bar plot
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, before_normalization, width, label='Sebelum Normalisasi')
rects2 = ax.bar(x + width/2, after_normalization, width, label='Setelah Normalisasi')

# Menambahkan label, judul, dan legend
ax.set_ylabel('Akurasi')
ax.set_title('Perbandingan Akurasi Model Sebelum dan Sesudah Normalisasi')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Menambahkan label nilai di atas setiap bar
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

# Menampilkan plot
fig.tight_layout()
plt.show()

"""# Kesimpulan

##Tingkat Akurasi Setiap Algoritma

Algoritma	Akurasi (Sebelum Normalisasi)	Akurasi (Setelah Normalisasi)

- Naive Bayes	63.1%	(sebelum) 37.2% (sesudah)
- Decision Tree	57.8%	(sebelum) 62.8% (sesudah)
- Random Forest	67.8%	(sebelum) 37.2% (sesudah)

##Keunggulan dan Keterbatasan Setiap Algoritma
Naive Bayes
Keunggulan:

Sederhana dan cepat.
- Efektif untuk dataset dengan dimensi tinggi.

Keterbatasan:
- Membutuhkan asumsi independensi antar fitur.
- Rentan terhadap fitur yang tidak relevan.

Decision Tree
Keunggulan:

- Mudah dipahami dan diinterpretasikan.
- Mampu menangani data kategorikal maupun numerik.

Keterbatasan:
- Rentan terhadap overfitting.
- Sensitif terhadap perubahan data (tidak stabil).

Random Forest
Keunggulan:
- Mengurangi risiko overfitting.
- Tahan terhadap data yang memiliki missing value dan outlier.
- Lebih stabil dibandingkan metode lain.

Keterbatasan:
- Lebih kompleks secara komputasi.
- Membutuhkan waktu pelatihan yang relatif lebih lama.

##Rekomendasi Algoritma
Algoritma Random Forest direkomendasikan untuk kasus ini karena:

Akurasi Tertinggi: Mencapai akurasi hampir 68%, melampaui algoritma Naive Bayes dan Decision Tree.
Tahan dan Stabil: Lebih andal dalam memprediksi data baru serta tidak mudah terpengaruh oleh outlier.
Kemampuan Generalisasi yang Baik: Dapat diterapkan pada dataset lain dengan karakteristik serupa.
"""

filename = 'model_naive_bayes.pkl'
pickle.dump(clean_classifier_nb, open(filename, 'wb'))